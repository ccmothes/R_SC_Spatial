---
title: "R for Spatial Analysis, Breify"
author: "dan carver"
date: "6/15/2020"
output:
  html_document: default
---

## Objective 
- introduce spatial libraries (raster,rgdal, sp, tmap)
- read in raster data
- create spatial datasets from a csv
- perform basic spatail analysis 
- produce interactive maps via rmd 

## Spatial Libraries in R 
There are multiple software carpentry lessons that focus on spatial data analysis here. Please use these resources for full examples of geospatial R. 

[Data Carpentries Geospatial Workshops](https://datacarpentry.org/lessons/#geospatial-curriculum)

- Introduction to R for Geospatial Data

- Introduction to Geospatial Raster and Vector Data with R

Both discuss **dplyr** manipulation of spatial datasets. They are still being develop so there are opportunities for you to submit comments and suggestions. Live the open source life. 

### Geospatial Data Abstraction Library 
[GDAL](https://gdal.org/index.html) is the base library for nearly all spatial data analysis in any computer language. ERSI functions are based on it, QGIS functions are based on it. R spatial libraries are based on it. It's very important to know this, but you almost never engage with it directly. 

We access GDAL through the **rgdal** library, which we will install now. 
```{r eval=FALSE, warning = FALSE}
# install.packages("rgdal")
library(rgdal)
```
<br>

If we look at the functions within the library, things appear a little cryptic. 
The most readily used function is probably **writeOGR()** which allows you to write out spatial data. 


Because GDAL is so foundational, we don't often engage with the functions directly. We rely on other spatial libraries which make that connection for us. **raster** for manipulating grid based datasets.  **sp** or **sf** for vector based processing. There are numerous others for more specific applications. 
<br>

>**to SP or SF, that is the question** The honest answer is both. "sp" was established in 2005 and is still the backbone for many other spatial libraries. "sf" is a newer library and is expect to replace sp over time. Maybe the best way to describe the difference is that "sp" creates a spatial object with a dataframe attached. "sf" creates a dataframe with spatial object attached. This means that "sf" objects can be manipulated using the **tidyverse**, which means they behave a bit more like we expect objects in R to behave. 
That said, there are time when you need a "sp" object or a "sf" object to enable a specific method to work. I find I lean on "sp", because it's the first one I worked with. Luckly converting between the object types is easy with functions like **st_to_sf**. 

<br>

### Install a Few Spatial Packages 

```{r eval = FALSE, warning = FALSE}
#install.packages("raster")
#install.packages("sp")
library(raster)
library(sp)
```

<br>

### Loading a raster into R 

We will rely on the raster library to bring in our first bit of spatial data. 
<br>

```{r eval = FALSE, warning = FALSE}
# set base director, which defines where your files are store/written
baseDir <- "path/"

# read in the file
proLands <- raster::raster(x = paste0(baseDir,"proAreas.tif"))
# print it out to view some of the metadata 
proLands 
#quickly visualize the content 
plot(proLands)
```
<br>

We can view much of the meta data associated with the file just by printing it. Content such as **extent** or **resolution** can be selected directly using the **raster@extent** indexing. This is a special type of indexing that is connect to more complex data types(S4 objects) in R
<br>


### Generate Spatial Data from a csv 
<br>

```{r eval = FALSE, warning = FALSE}

# read in the data 
d1 <- read.csv(paste0(baseDir,"cucurbitaData.csv")
str(d1)
#view the unique species 
unSpec <- unique(d1$taxon)
unSpec

# subset all records assocaited with a species 
d2 <- d1[d1$taxon == unSpec[1],]

```
<br>

With the dataframe clean up we will use the **sp** library to generate a spatial object. 
<br>

```{r eval = FALSE, warning = FALSE}
# generate a spatial point dataframe 
# coords = df of longitude , latitude (x,y) values 
# data = information associate with the records 
# proj4string = 
sp1 <- sp::SpatialPointsDataFrame(coords = d2[,c(4,3)],
                                  data = d2,
                         proj4string = p1@crs)
# view the object 
sp1

# view the data 
View(sp1@data)

plot(sp1)
```
<br>

The plot of the spatial point object does not tell us much. Lets load in a library specifically created for mapping spatial objects to better visual the points 

<br>

```{r eval = FALSE, warning = FALSE}
install.packages("tmap")
library(tmap)
# use the qick map function to visualize the data 
tmap::qtm(sp1)
```
<br>

Ok, so that didn't change much. We will come back to this and so how to add come complexity and interactivity to our maps with **tmap**. 

<br>

### Comparing the data 

<br>

Before we set into an analysis using the two datasets we will evaluate how well they align 
<br>

```{r eval = FALSE, warning = FALSE}
# check extent 
extent(proLands) == extent(sp1)
extent(proLands) > extent(sp1)

# check coordinate reference system 
raster::compareCRS(x = proLands, y = sp1)

```

<br>

As the extents are different it's worth while to clip the bigger dataset to improve processing time and keep a clean study area.  
<br>

### Crop the data 

<br>

```{r eval = FALSE, warning = FALSE}
# Crop the protected areas to the extent of the points 
p1Crop <- raster::crop(x = proLands, y = sp1)

#quick visual check with qtm 
qtm(proLands)

qtm(p1Crop)

```
<br>


The extent has shrunk quite a bit. Now that know the datasets match spatially we can conduct an analysis between them. 

<br>`

### Extracting values to points 

<br>

We will use our points and protect lands raster to determine which if any of the occurrence are found within protect lands. 
<br>

```{r eval = FALSE, warning = FALSE}
# extract values 
?extract

# extract returns a vector of length(y), therefore we can just at that 
# data as new column to our spatial points dataframe 
sp1$inProArea <- raster::extract(x = p1Crop, y = sp1)

View(sp1@data)

```
<br>

With the result recorded in the point data we can visualize the values on a map. 
<br>

### Map the data 

<br>

```{r eval = FALSE, warning = FALSE}
# map the points 
map <- tm_shape(shp = sp1)+
  tm_dots(col = "inProArea", size = 0.1,title = "Occurrences in Protected Areas")
map
```
<br>

```{r eval = FALSE, warning = FALSE}
# add the raster to the map 
map2 <- map +
  tm_shape(p1Crop) +
    tm_raster(alpha = 0.4,palette = "green", title = "Protected Areas")
map2
```
<br>



```{r eval = FALSE, warning = FALSE}
tmap::tmap_save(tm = map2, filename = paste0(baseDir,"map.png")
# set the map to interactive and adjust the base map 
tmap::tmap_mode("view")
map2 + tm_basemap("OpenStreetMap")
```
<br>

## challenge 
<br>
Use a for loop and a function to generate the protected points map for each species in the species list. 
<br>

```{r eval = FALSE, warning = FALSE}
#hint 
for(i in speciesList){
  map <- function(i){
    # subset data 
    # read in raster 
    # crop raster
    # extract values 
    # map content 
    return(map)
  }
  tmap::tmap_save(tm = map, filename = paste0(i,"_protectedOccurrences.png"))
}
```


